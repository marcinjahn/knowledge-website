(window.webpackJsonp=window.webpackJsonp||[]).push([[126],{610:function(e,t,s){"use strict";s.r(t);var a=s(22),o=Object(a.a)({},(function(){var e=this,t=e.$createElement,s=e._self._c||t;return s("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[s("h1",{attrs:{id:"storage"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#storage"}},[e._v("#")]),e._v(" Storage")]),e._v(" "),s("p",[e._v("Stateful applications require storage to be attached to its pods.")]),e._v(" "),s("h2",{attrs:{id:"volumes"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#volumes"}},[e._v("#")]),e._v(" Volumes")]),e._v(" "),s("p",[s("img",{attrs:{src:"https://i.imgur.com/nPKhzLL.png",alt:""}})]),e._v(" "),s("p",[s("img",{attrs:{src:"https://i.imgur.com/qMFmqlN.png",alt:""}})]),e._v(" "),s("p",[e._v("Volume types:")]),e._v(" "),s("ul",[s("li",[e._v("emptyDir - a simple directory. Preserved for the pod existence time. Multiple\ncontainers in a pod can share data this way. It can be backed by a dir on a\nnode, or tmpfs on a node")]),e._v(" "),s("li",[e._v("hostPath - worker node's path. It's dangerous, because pod may get access to\nall files on the node.")]),e._v(" "),s("li",[e._v("nfs")]),e._v(" "),s("li",[e._v("azureFile")]),e._v(" "),s("li",[e._v("azureDisk")]),e._v(" "),s("li",[e._v("persistentVolumeClaim")]),e._v(" "),s("li",[e._v("...")])]),e._v(" "),s("p",[e._v("Different types have different configuration options. E.g., "),s("code",[e._v("emptyDir")]),e._v(' can have\n"medium" and "sizeLimit" configured.')]),e._v(" "),s("p",[e._v("Volumes can be mounted as read/write, or read-only. In multi-mount situations,\ndepending on the provider, only one, or many pods can write to the same volume.")]),e._v(" "),s("h3",{attrs:{id:"persistent-volumes"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#persistent-volumes"}},[e._v("#")]),e._v(" Persistent Volumes")]),e._v(" "),s("p",[e._v("The direct volume mounting described above is problematic, because it exposes\nthe details about the volume to pod/deployment definition (e.g., I'd have to\nspecify the URL of NFS storage there, or I have to specify that Azure infra is\nused for storage). This makes the manifests tied to a specific cloud provider.")]),e._v(" "),s("p",[s("img",{attrs:{src:"https://i.imgur.com/tH4mCzX.png",alt:""}})]),e._v(" "),s("p",[e._v("Additional objects were introduced to abstract the storage info away:")]),e._v(" "),s("p",[s("img",{attrs:{src:"https://i.imgur.com/vN89M9j.png",alt:""}})]),e._v(" "),s("p",[e._v("The volume providing is divided between two personas:")]),e._v(" "),s("ul",[s("li",[e._v("cluster admins - defines available "),s("em",[e._v("PersistentVolumes")])]),e._v(" "),s("li",[e._v("cluster user/dev - requests storage for their app using\n"),s("em",[e._v("PersistentVolumeClaim")]),e._v(".")])]),e._v(" "),s("p",[e._v("Storage objects:")]),e._v(" "),s("ul",[s("li",[s("strong",[e._v("Persistent Volume")]),e._v(" - represents a storage volume; it stores information\nabout it;")]),e._v(" "),s("li",[s("strong",[e._v("Persistent Volume Claim")]),e._v(" - represents user's claim on the persistent\nvolume. Its lifecycle is not tied to that of a pod, so the ownership of the\npersistent volume is decoupled from the pod. To use a persistent volume, user\nfirst needs to claim it. When the volume is no longer needed, the user\nreleases it by deleting the claim object.")])]),e._v(" "),s("p",[e._v("To use the volume, pod needs to refer to a PersistentVolumeClaim that is bound\nto a PersistentVolume. The claim might either reference exact name of the\nPersistentVolume, or just list requirements to allow K8s to bind any fitting\nPersistentVolume to it.")]),e._v(" "),s("p",[e._v("Multiple pods can use the same volume by referencing the same claim. Depending\non the storage type, access will be enabled or not. E.g., some storage providers\nallow writing from one node only (so multiple pods on that node can write). Other\nnodes cannot write.")]),e._v(" "),s("p",[e._v('PV that is ready to be claimed has "Available" status.')]),e._v(" "),s("h4",{attrs:{id:"reclaim-policies"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#reclaim-policies"}},[e._v("#")]),e._v(" Reclaim Policies")]),e._v(" "),s("p",[e._v("PV can configure its reclaim policy:")]),e._v(" "),s("ul",[s("li",[s("strong",[e._v("Retain")]),e._v(' - when PVC is deleted, the PV becomes "Released", the volume is\nretained. Admin must manually remove it. If a PV is claimed, and then the PVC\nis deleted, the PV\'s Status becomes "Released", it cannot be claimed in this\nstate. It can be claimed again if the object gets edited and the\n'),s("code",[e._v(".spec.claimRef")]),e._v(" gets removed (or the whole PV needs to be deleted and then\nrecreated). If PV gets deleted, the data stays intact.")]),e._v(" "),s("li",[s("strong",[e._v("Delete")]),e._v(" - when PVC is deleted, the PV and the underlying data are deleted\nas well. It is used with the automatically provisioned PVs.")])]),e._v(" "),s("p",[e._v("PV is just a pointer to the data.")]),e._v(" "),s("h4",{attrs:{id:"access-modes"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#access-modes"}},[e._v("#")]),e._v(" Access Modes")]),e._v(" "),s("p",[e._v("Access Modes of a PersistentVolume refer to how many nodes can mount it (not\npods). Types:")]),e._v(" "),s("ul",[s("li",[s("em",[e._v("ReadWriteOnce")]),e._v(" - only one node can mount it in read/write mode. Others can't\nmount it at all.")]),e._v(" "),s("li",[s("em",[e._v("ReadOnlyMany")]),e._v(" - many nodes can mount it in read-only mode")]),e._v(" "),s("li",[s("em",[e._v("ReadWriteMany")]),e._v(" - many nodes can mount it in read/write mode.")])]),e._v(" "),s("p",[e._v("A PersistentVolume can support multiple modes.")]),e._v(" "),s("h4",{attrs:{id:"deletion"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#deletion"}},[e._v("#")]),e._v(" Deletion")]),e._v(" "),s("p",[e._v("Deleting a PV will await for a bound PVC to be deleted. Deleting a PVC will\nawait for a bound pod to be deleted.")]),e._v(" "),s("h4",{attrs:{id:"auto-provisioning"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#auto-provisioning"}},[e._v("#")]),e._v(" Auto Provisioning")]),e._v(" "),s("p",[e._v("PersistentVolumes might also be created automatically on-demand as needed, if\nthe automated provisioner is installed (e.g. AKS creates Azure Disk\nautomatically when it's needed).")]),e._v(" "),s("p",[s("img",{attrs:{src:"https://i.imgur.com/UT7ZnCU.png",alt:""}})]),e._v(" "),s("p",[e._v("Here, the order gets reversed. Instead of creating a PVC for a pre-existing PV,\na PVC gets created first, and then a proper PV gets created by a provisioner.")]),e._v(" "),s("h5",{attrs:{id:"storage-class"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#storage-class"}},[e._v("#")]),e._v(" Storage Class")]),e._v(" "),s("p",[e._v("Cloud provider offers some "),s("em",[e._v("StorageClasses")]),e._v(" (e.g. Azure offers AzureDisk and\nAzureFile). A PVC should contain information which StorageClass it expects.")]),e._v(" "),s("p",[e._v("The PVC uses "),s("code",[e._v("storageClassName")]),e._v(" to choose Storage Class. If it's ommited, cloud\nproviders have some default choice. Setting "),s("code",[e._v("storageClassName")]),e._v(" to "),s("code",[e._v('""')]),e._v(" disables\ndynamic provisioning and causes an existing PV to be selected for binding.")]),e._v(" "),s("p",[e._v('Depending on K8s implementation, creating new PVC with some storageClass will\ninstantiate the PV immediately, or only after some pod actually will need some\nstorage (using our PVC). E.g., GKE will create PV immediately, while kind will\nwait for pod. This is because kind create local storage and it needs to know\nwhere a pod will be scheduled (on which node) to create the storage there. The\nbehaviour is controlled with the "volume binding mode" config of a storage\nclass.')]),e._v(" "),s("p",[e._v("We can define our own storageClasses.")])])}),[],!1,null,null,null);t.default=o.exports}}]);